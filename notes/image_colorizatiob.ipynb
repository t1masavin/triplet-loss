{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here, in this notebook, I have tried to implement the famous <b>pix2pix</b> research paper for the <b>Image colorization</b> task with <b>additional pretraining</b> to the Generator in terms of replacing it's backbone with a resnet pretrained on ImageNet followed by additional pretraining for colorization in a supervised manner using L1 loss. This pretraining has been done to deal with the fact that we will be using a much <b>smaller dataset</b> than the original paper.\n",
    "\n",
    "pix2pix, proposed a general solution to many image-to-image tasks in deep learning which one of those was colorization. In this approach two losses are used: L1 loss, which makes it a regression task, and an adversarial (GAN) loss, which helps to solve the problem in an unsupervised manner.</p>\n",
    "\n",
    "**Loss Function to be optimized**\n",
    "\n",
    "$G^*=\\arg \\min _G \\max _D \\mathcal{L}_{c G A N}(G, D)+\\lambda \\mathcal{L}_{L 1}(G)$\n",
    "\n",
    "**L1 loss** \n",
    "\n",
    "$\\mathcal{L}_{L 1}(G)=\\mathbb{E}_{x, y, z}\\left[\\|y-G(x, z)\\|_1\\right]$\n",
    "\n",
    "**GAN Loss**\n",
    "\n",
    "$\\begin{aligned} \\mathcal{L}_{c G A N}(G, D)=& \\mathbb{E}_{x, y}[\\log D(x, y)]+\\\\ & \\mathbb{E}_{x, z}[\\log (1-D(x, G(x, z))]\\end{aligned}$\n",
    "\n",
    "* x -> grayscale image (the condition introduced)\n",
    "* y -> 2 channel output of generator\n",
    "* z -> input noise of generator\n",
    "* G -> Generator Model\n",
    "* D -> Discriminator Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = \"center\"> üìö Theory </h1>\n",
    "\n",
    "## üí° Basics of GANs\n",
    "\n",
    "<p>2 types of models involved, <b>a generative model and a discriminative model</b></p>\n",
    "\n",
    "* The generative model tries to find the joint probability P(X,Y) or P(X) when there are no labels where X is the set of data instances and Y is the set of labels. Then the joint probability can be used to find P(Y|X) or P(X|Y).\n",
    "* The discriminative model tries to find the conditional probability P(Y|X) directly.\n",
    "\n",
    "<p>Discriminative models try to draw boundaries in the data space, while generative models try to model how data is placed throughout the space.</p>\n",
    "\n",
    "\n",
    "## üë®‚Äçüè≠ Basic Working of GANs\n",
    "\n",
    "<p>Both the generator and the discriminator are neural networks. The generator output is connected directly to the discriminator input. Through backpropagation, the discriminator's classification provides a signal that the generator uses to update its weights.\n",
    "\n",
    "A generative adversarial network (GAN) has two parts:</p>\n",
    "\n",
    "* The generator learns to generate plausible data. The generated instances become negative training examples for the discriminator.\n",
    "* The discriminator learns to distinguish the generator's fake data from real data. The discriminator penalizes the generator for producing implausible results.\n",
    "\n",
    "## ‚úî The Discriminator\n",
    "\n",
    "<p>The discriminator in a GAN is simply a classifier. It tries to distinguish real data from the data created by the generator. It could use any network architecture appropriate to the type of data it's classifying.</p> \n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/gan/images/gan_diagram_discriminator.svg)\n",
    "\n",
    "### Training process for the Discriminator\n",
    "\n",
    "* The Training data for the discriminator include Real data (positive examples) and Fake data (negative examples generated by the generator). \n",
    "\n",
    "* During discriminator training the generator does not train. Its weights remain constant while it produces examples for the discriminator to train on. The discriminator connects to two loss functions. During discriminator training, the discriminator ignores the generator loss and just uses the discriminator loss.\n",
    "\n",
    "1. The discriminator classifies both real data and fake data from the generator.\n",
    "2. The discriminator loss penalizes the discriminator for misclassifying a real instance as fake or a fake instance as real.\n",
    "3. The discriminator updates its weights through backpropagation from the discriminator loss through the discriminator network.\n",
    "\n",
    "## ‚öó The Generator\n",
    "\n",
    "<p>The generator part of a GAN learns to create fake data by incorporating feedback from the discriminator. It learns to make the discriminator classify its output as real.</p>\n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/gan/images/gan_diagram_generator.svg)\n",
    "\n",
    "The generator training includes:\n",
    "* Random input noise to the generator.\n",
    "* The generator network that generates the new data instances.\n",
    "* The discriminator network the classifies the generator output.\n",
    "* The generator loss that penalizes the generator for failing to fool the generator.\n",
    "\n",
    "\n",
    "The Steps involved are:\n",
    "1. Sample Random Noise \n",
    "2. Produce generaator output from the sampled random noise\n",
    "3. Get the discriminator classification for the generator output\n",
    "4. Calculate loss from discriminator output.\n",
    "5. Backpropogate through both discriminator and generator and obtain the gradients, only updating the generator weights.\n",
    "\n",
    "## GAN Training\n",
    "\n",
    "<p>The GAN has to juggle the training of 2 networks and the convergence is hard to identify.\n",
    "\n",
    "The training proceeds in the following manner:</p>\n",
    "1. The discriminator trains for one or more epochs\n",
    "2. The generator trains for one or more epochs\n",
    "3. Repeat steps 1 and 2\n",
    "\n",
    "## üìâ Loss Functions\n",
    "\n",
    "<p>GANs try to replicate a probability distribution. They should therefore use loss functions that reflect the distance between the distribution of the data generated by the GAN and the distribution of the real data.</p>\n",
    "\n",
    "### Minimax Loss\n",
    "\n",
    "In the paper that introduced GANs, the generator tries to minimize the following function while the discriminator tries to maximize it:\n",
    "\n",
    "$\\begin{aligned} \\mathcal{L}_{G A N}(G, D)=& \\mathbb{E}_{x}[\\log D(x)]+\\\\ & \\mathbb{E}_{z}[\\log (1-D(G(z))]\\end{aligned}$\n",
    "\n",
    "In this function:\n",
    "\n",
    "* D(x) is the discriminator's estimate of the probability that real data instance x is real.\n",
    "* Ex is the expected value over all real data instances.\n",
    "* G(z) is the generator's output when given noise z.\n",
    "* D(G(z)) is the discriminator's estimate of the probability that a fake instance is real.\n",
    "* Ez is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)).\n",
    "\n",
    "The original GAN paper notes that the above minimax loss function can cause the GAN to get stuck in the early stages of GAN training when the discriminator's job is very easy. The paper therefore suggests modifying the generator loss so that the generator tries to maximize log D(G(z)). (**Modified Minimax Loss**)\n",
    "\n",
    "There are other loss functions too like the **Wasserstein Loss**.\n",
    "\n",
    "## ü•º LAB color space\n",
    "\n",
    "LAB is a color space just like RGB but here the 3 dimensions represent Lightness(L), Green-Redness (a) and Yellow-blueness (b) of each pixel\n",
    "\n",
    "The main advantages of using this color space over RGB for the Image Colorization task are:\n",
    "* Here, the L channel can straight away be used as the grayscale input of the image.\n",
    "* The model only needs to output 2 channels.\n",
    "* But if you use RGB, you have to first convert your image to grayscale, feed the grayscale image to the model and hope it will predict 3 numbers for you which is a way more difficult and unstable task due to the many more possible combinations of 3 numbers compared to two numbers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÉ Config Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    external_data_siae = 10000\n",
    "    train_size = 8000\n",
    "    image_size_1 = 256\n",
    "    image_size_2 = 256\n",
    "    batch_size = 32\n",
    "    LeakyReLU_slope = 0.2\n",
    "    dropout = 0.5\n",
    "    kernel_size = 4\n",
    "    stride = 2\n",
    "    padding =1\n",
    "    gen_lr = 2e-4\n",
    "    disc_lr = 2e-4\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.999\n",
    "    lambda_l1 = 100\n",
    "    gan_mode = 'vanilla'\n",
    "    layers_to_cut = -2\n",
    "    epochs = 20\n",
    "    pretrain_lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>üì¶ Importing Packages</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.models.resnet import resnet18\n",
    "from torchvision.models.vgg import vgg19\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAI imports + Loading in the Data\n",
    "\n",
    "The data is a small sample of the COCO dataset for object detection and is loaded in through the external data functionality of fastai. The actual paper uses the entire ImageNet dataset while we only use 8000 training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/tim/.fastai/data/coco_sample')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.vision.learner import create_body\n",
    "from fastai.vision.models.unet import DynamicUnet\n",
    "from fastai.data.external import untar_data, URLs\n",
    "\n",
    "path = untar_data(URLs.COCO_SAMPLE)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69af19485ccd259f20d08d91c8b9e13943707c152c9e0686319dd8e8141c9bef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

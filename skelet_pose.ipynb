{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/miniconda3/envs/geo/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/tim/miniconda3/envs/geo/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pytorchvideo\n",
    "\n",
    "import json \n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ") \n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup  \n",
    "Download the id to label mapping for the Kinetics 400 dataset on which the Torch Hub models were trained. This will be used to get the category label names from the predicted class ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model using Torch Hub API  \n",
    "PyTorchVideo provides several pretrained models through Torch Hub. Available models are described in model zoo documentation.  \n",
    "\n",
    "Here we are selecting the slowfast_r50 model which was trained using a 8x8 setting on the Kinetics 400 dataset.  \n",
    "\n",
    "NOTE: to run on GPU in Google Colab, in the menu bar selet: Runtime -> Change runtime type -> Harware Accelerator -> GPU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tim/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cuda\"\n",
    "\n",
    "# Pick a pretrained model \n",
    "model_name = \"slowfast_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo:main\", model=model_name, pretrained=True)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.666666666666664"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# SlowFast transform\n",
    "####################\n",
    "\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 16\n",
    "frames_per_second = 12\n",
    "alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "clip_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the example video\n",
    "# video_path = \"porevo.mp4\"  \n",
    "\n",
    "# # Select the duration of the clip to load by specifying the start and end duration\n",
    "# # The start_sec should correspond to where the action occurs in the video\n",
    "# start_sec = 90\n",
    "# end_sec = start_sec + clip_duration \n",
    "\n",
    "# # Initialize an EncodedVideo helper class\n",
    "# video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# # Load the desired clip\n",
    "# video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "# # Apply a transform to normalize the video input\n",
    "# video_data = transform(video_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Move the inputs to the desired device\n",
    "# inputs = video_data[\"video\"]\n",
    "# inputs = [i.to(device)[None, ...] for i in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pass the input clip through the model \n",
    "# preds = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the predicted classes \n",
    "# post_act = torch.nn.Softmax(dim=1)\n",
    "# preds = post_act(preds)\n",
    "# pred_classes = preds.topk(k=5).indices\n",
    "\n",
    "# # Map the predicted classes to the label names\n",
    "# pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\n",
    "# print(\"Predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_video():\n",
    "    for i in range(10):\n",
    "        # Load the example video\n",
    "        video_path = \"porevo.mp4\"  \n",
    "\n",
    "        # Select the duration of the clip to load by specifying the start and end duration\n",
    "        # The start_sec should correspond to where the action occurs in the video\n",
    "        start_sec = 90 + clip_duration * i\n",
    "        end_sec = start_sec + clip_duration \n",
    "\n",
    "        # Initialize an EncodedVideo helper class\n",
    "        video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "        # Load the desired clip\n",
    "        video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "        # Apply a transform to normalize the video input\n",
    "        video_data = transform(video_data)\n",
    "\n",
    "        # Move the inputs to the desired device\n",
    "        inputs = video_data[\"video\"]\n",
    "        inputs = [i.to(device)[None, ...] for i in inputs]\n",
    "        # Pass the input clip through the model \n",
    "        preds = model(inputs)\n",
    "        # Get the predicted classes \n",
    "        post_act = torch.nn.Softmax(dim=1)\n",
    "        preds = post_act(preds)\n",
    "        pred_classes = preds.topk(k=5).indices\n",
    "\n",
    "        # Map the predicted classes to the label names\n",
    "        pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\n",
    "        print(\"Predicted labels: %s\" % \", \".join(pred_class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: taking a shower, shaving legs, trimming or shaving beard, washing hair, shaving head\n",
      "Predicted labels: shaving legs, washing hair, situp, getting a haircut, shaving head\n",
      "Predicted labels: tickling, shaving legs, situp, washing hair, laughing\n",
      "Predicted labels: tickling, shaving legs, washing hair, getting a tattoo, kissing\n",
      "Predicted labels: washing hair, tickling, washing feet, shaving legs, kissing\n",
      "Predicted labels: shaving legs, washing hair, washing feet, kissing, tickling\n",
      "Predicted labels: trimming or shaving beard, applying cream, waxing chest, eating burger, waxing back\n",
      "Predicted labels: yoga, shaving legs, eating hotdog, waxing legs, massaging feet\n",
      "Predicted labels: yoga, trimming or shaving beard, drinking beer, shaving legs, eating hotdog\n",
      "Predicted labels: trimming or shaving beard, shaving head, drinking beer, brush painting, getting a haircut\n"
     ]
    }
   ],
   "source": [
    "do_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69af19485ccd259f20d08d91c8b9e13943707c152c9e0686319dd8e8141c9bef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
